{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkXaFeLXTKXtDeEUTIvAxa"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Custom Destinations and Reverse ETL**\n",
        "\n",
        "Using dlt's `@dlt.destination` you can send data various destinations other than the usual ones like Postgres or BigQuery, for instance you can send data to\n",
        "- APIs like notion, slack\n",
        "- Message queues\n",
        "- Logging systems\n",
        "- Custom data sinks\n",
        "\n",
        "This is useful for Reverse ETL\n",
        "\n",
        "```python\n",
        "@dlt.destination(\n",
        "    batch_size=10, # how many items per function call are batched together\n",
        "    loader_file_format=\"jsonl\", # in which format should the files be LOADED\n",
        "    name=\"my_custom_destination\", # a custom name to the destination\n",
        "    naming_convention=\"direct\", # controls how table and column names are normalised, direct means keep all names same\n",
        "    max_table_nesting=0, # unnesting of nested fields, here 0 means no unnesting\n",
        "    skip_dlt_columns_and_tables=True, # defines whether internal tables and columns will be fed into the custom destination\n",
        "    max_parallel_load_jobs=5,\n",
        "    loader_parallelism_strategy=\"table-sequential\",\n",
        ")\n",
        "def my_destination(items: TDataItems, table: TTableSchema) -> None:\n",
        "    ...\n",
        "```"
      ],
      "metadata": {
        "id": "0XJdlxdWEbRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pymysql duckdb dlt"
      ],
      "metadata": {
        "id": "rHtpffhcEX_B"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Creating a Simple Print Output Custom Destination"
      ],
      "metadata": {
        "id": "fwtp9Z3eEP4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dlt\n",
        "from dlt.common.typing import TDataItems # A single data item or a list as extracted from the data source\n",
        "from dlt.common.schema import TTableSchema # TypedDict that defines properties of a table\n",
        "\n",
        "@dlt.destination(batch_size=5)\n",
        "def print_sink(items: TDataItems, table: TTableSchema):\n",
        "  print(f\"\\nTable: {table['name']}\")\n",
        "  for item in items:\n",
        "    print(item)\n",
        "\n",
        "@dlt.resource\n",
        "def simple_data():\n",
        "    yield [{\"id\": i, \"value\": f\"row-{i}\"} for i in range(10)]\n",
        "\n",
        "\n",
        "pipeline = dlt.pipeline(\"print_example\", destination=print_sink)\n",
        "load_info = pipeline.run(simple_data())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3VN-8UZEVTw",
        "outputId": "a45d5fd3-ec53-43eb-e9f9-d9920b001cdc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Table: simple_data\n",
            "{'id': 0, 'value': 'row-0'}\n",
            "{'id': 1, 'value': 'row-1'}\n",
            "{'id': 2, 'value': 'row-2'}\n",
            "{'id': 3, 'value': 'row-3'}\n",
            "{'id': 4, 'value': 'row-4'}\n",
            "\n",
            "Table: simple_data\n",
            "{'id': 5, 'value': 'row-5'}\n",
            "{'id': 6, 'value': 'row-6'}\n",
            "{'id': 7, 'value': 'row-7'}\n",
            "{'id': 8, 'value': 'row-8'}\n",
            "{'id': 9, 'value': 'row-9'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*We can see that the data was loaded to the custom destination in two batches as specified*"
      ],
      "metadata": {
        "id": "R6C6sovVG6lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dlt.destination(batch_size=2)\n",
        "def print_sink(items: TDataItems, table: TTableSchema):\n",
        "  print(f\"\\nTable: {table['name']}\")\n",
        "  for item in items:\n",
        "    print(item)\n",
        "\n",
        "@dlt.resource\n",
        "def simple_data():\n",
        "    yield [{\"id\": i, \"value\": f\"row-{i}\"} for i in range(6)]\n",
        "\n",
        "\n",
        "pipeline = dlt.pipeline(\"print_example\", destination=print_sink)\n",
        "load_info = pipeline.run(simple_data())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URJIJaptnkPn",
        "outputId": "2e61b6b6-db22-4aa1-a3ba-87f7f212179a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Table: simple_data\n",
            "{'id': 0, 'value': 'row-0'}\n",
            "{'id': 1, 'value': 'row-1'}\n",
            "\n",
            "Table: simple_data\n",
            "{'id': 2, 'value': 'row-2'}\n",
            "{'id': 3, 'value': 'row-3'}\n",
            "\n",
            "Table: simple_data\n",
            "{'id': 4, 'value': 'row-4'}\n",
            "{'id': 5, 'value': 'row-5'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Creating Custom Destination - Notion Database\n",
        "\n",
        "### 2.1. Initialse Database in Notion\n",
        "Create a database in Notion iwth columns\n",
        "Accession (title), ID (text), Description (text)\n"
      ],
      "metadata": {
        "id": "C5lD40ReFyjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Install Necessary Libraries and Fetch Notion Secret Key\n",
        "\n",
        "1. Go to https://www.notion.so/profile/integrations\n",
        "2. Create a new internal integrations\n",
        "3. Copy the key and store in google colab's user data\n",
        "4. Make sure the connect the page with the database with the integration created (On the page > options > connections > select created integration)"
      ],
      "metadata": {
        "id": "SdZse3ZcHemZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install dlt pymysql notion-client"
      ],
      "metadata": {
        "id": "LcxZRNukG0Qj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Test if your Notion integration works, you should receive 200 as status code*"
      ],
      "metadata": {
        "id": "S-TRLzMPhZAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from google.colab import userdata\n",
        "\n",
        "url = \"https://api.notion.com/v1/search\"\n",
        "\n",
        "os.environ[\"NOTION_SECRET\"] = userdata.get(\"NOTION_SECRET\")\n",
        "\n",
        "# Load token from environment variable\n",
        "notion_token = os.getenv(\"NOTION_SECRET\")\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {notion_token}\",\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Notion-Version\": \"2022-06-28\"\n",
        "}\n",
        "\n",
        "data = {\n",
        "    \"query\": \"dlt\",\n",
        "    \"filter\": {\n",
        "        \"value\": \"database\",\n",
        "        \"property\": \"object\"\n",
        "    },\n",
        "    \"sort\": {\n",
        "        \"direction\": \"ascending\",\n",
        "        \"timestamp\": \"last_edited_time\"\n",
        "    }\n",
        "}\n",
        "\n",
        "response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "print(response.status_code)\n",
        "print(response.json())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcLfEvpoguV2",
        "outputId": "f1996f7d-4e82-4a27-ff71-d76ba8ee35ac"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n",
            "{'object': 'list', 'results': [{'object': 'database', 'id': '1df9c4c8-7d7b-8000-be1b-e17d187a1b6e', 'cover': None, 'icon': None, 'created_time': '2025-04-24T15:47:00.000Z', 'created_by': {'object': 'user', 'id': '6fc75fc3-822e-42c0-9b58-cd1bff0ae559'}, 'last_edited_by': {'object': 'user', 'id': '6fc75fc3-822e-42c0-9b58-cd1bff0ae559'}, 'last_edited_time': '2025-04-24T16:10:00.000Z', 'title': [{'type': 'text', 'text': {'content': 'dlt Advanced Course', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'dlt Advanced Course', 'href': None}], 'description': [], 'is_inline': False, 'properties': {'ID': {'id': '%5Cry%5C', 'name': 'ID', 'type': 'rich_text', 'rich_text': {}}, 'Description': {'id': 'pD%5Ex', 'name': 'Description', 'type': 'rich_text', 'rich_text': {}}, 'Accession': {'id': 'title', 'name': 'Accession', 'description': '', 'type': 'title', 'title': {}}}, 'parent': {'type': 'page_id', 'page_id': '1df9c4c8-7d7b-8098-abdf-eae6c302252a'}, 'url': 'https://www.notion.so/1df9c4c87d7b8000be1be17d187a1b6e', 'public_url': None, 'archived': False, 'in_trash': False}], 'next_cursor': None, 'has_more': False, 'type': 'page_or_database', 'page_or_database': {}, 'request_id': 'e5dc1120-09ce-4aa8-a6c8-b1fff852e2a5'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Load Data from MySQL Public Data"
      ],
      "metadata": {
        "id": "HscagHc1Jnvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import dlt\n",
        "from dlt.sources.sql_database import sql_database\n",
        "\n",
        "import sqlalchemy as sa\n",
        "from sqlalchemy import text\n",
        "from dlt.sources.sql_database import sql_database\n",
        "\n",
        "# using a query adapter to limit the number of rows ingested\n",
        "def limit_rows(query, table):\n",
        "    return text(f\"SELECT * FROM {table.fullname} LIMIT 20\")\n",
        "\n",
        "\n",
        "source = sql_database(\n",
        "    \"mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam\",\n",
        "    table_names=[\"family\",],\n",
        "    query_adapter_callback=limit_rows\n",
        ")"
      ],
      "metadata": {
        "id": "D57vXHCFJQLj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4. Create Notion Custom Destination"
      ],
      "metadata": {
        "id": "juITgj7GVzmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from notion_client import Client\n",
        "\n",
        "\n",
        "os.environ[\"DESTINATION__NOTION__NOTION_AUTH\"] = userdata.get('NOTION_SECRET')\n",
        "os.environ[\"DESTINATION__NOTION__NOTION_PAGE_ID\"] = userdata.get('NOTION_PAGE_ID')\n",
        "\n",
        "@dlt.destination(name=\"notion\")\n",
        "def push_to_notion(items, table, notion_auth=dlt.secrets.value, notion_page_id=dlt.secrets.value):\n",
        "  client = Client(auth=notion_auth)\n",
        "  print(len(items))\n",
        "\n",
        "  for item in items:\n",
        "    client.pages.create(\n",
        "        parent={\"database_id\": notion_page_id},\n",
        "        properties={\n",
        "                \"Accession\": {\"title\": [{\"text\": {\"content\": item[\"rfam_acc\"]}}]},\n",
        "                \"ID\": {\"rich_text\": [{\"text\": {\"content\": item[\"rfam_id\"]}}]},\n",
        "                \"Description\": {\"rich_text\": [{\"text\": {\"content\": item[\"description\"]}}]}\n",
        "            }\n",
        "    )"
      ],
      "metadata": {
        "id": "GYa6pX_EV6ml"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5. Time to Test the Custom Destination"
      ],
      "metadata": {
        "id": "yktC_uoQjwFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = dlt.pipeline(\"notion_pipeline\", destination=push_to_notion, progress=\"log\")\n",
        "pipeline.run(source, table_name=\"rfam_family\")\n",
        "print(pipeline.last_trace)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-O5UZmEWYLP",
        "outputId": "686d9e18-3700-4883-d420-a87814a6b4a2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-04-24 16:10:31,778|[WARNING]|143|134867654361088|dlt|pipeline.py|run:709|The pipeline `run` method will now load the pending load packages. The data you passed to the run function will not be loaded. In order to do that you must run the pipeline again\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------- Load sql_database in 1745510951.2799456 --------------------\n",
            "Jobs: 1/2 (50.0%) | Time: 0.00s | Rate: 182361.04/s\n",
            "Memory usage: 356.29 MB (12.50%) | CPU usage: 0.00%\n",
            "\n",
            "10\n",
            "10\n",
            "------------------- Load sql_database in 1745510951.2799456 --------------------\n",
            "Jobs: 1/2 (50.0%) | Time: 16.93s | Rate: 0.06/s\n",
            "Memory usage: 356.29 MB (12.80%) | CPU usage: 0.00%\n",
            "\n",
            "------------------- Load sql_database in 1745510951.2799456 --------------------\n",
            "Jobs: 2/2 (100.0%) | Time: 16.93s | Rate: 0.12/s\n",
            "Memory usage: 356.29 MB (12.80%) | CPU usage: 0.00%\n",
            "\n",
            "Run started at 2025-04-24 16:10:31.760527+00:00 and COMPLETED in 16.96 seconds with 2 steps.\n",
            "Step load COMPLETED in 16.94 seconds.\n",
            "Pipeline notion_pipeline load step completed in 16.93 seconds\n",
            "1 load package(s) were loaded to destination notion and into dataset None\n",
            "The notion destination used <dlt.common.configuration.specs.base_configuration.CredentialsConfiguration object at 0x7aa8f5ba4a50> location to store data\n",
            "Load package 1745510951.2799456 is LOADED and contains no failed jobs\n",
            "\n",
            "Step run COMPLETED in 16.96 seconds.\n",
            "Pipeline notion_pipeline load step completed in 16.93 seconds\n",
            "1 load package(s) were loaded to destination notion and into dataset None\n",
            "The notion destination used <dlt.common.configuration.specs.base_configuration.CredentialsConfiguration object at 0x7aa8f5ba4a50> location to store data\n",
            "Load package 1745510951.2799456 is LOADED and contains no failed jobs\n"
          ]
        }
      ]
    }
  ]
}