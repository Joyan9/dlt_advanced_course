{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPf1TbzD/JmpbZnjDQXNQSx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework: Speed up your pipeline**\n",
        "\n",
        "### **Goal**\n",
        "\n",
        "Use the public **Jaffle Shop API** to build a `dlt` pipeline and apply everything you've learned about performance:\n",
        "\n",
        "- Chunking\n",
        "- Parallelism\n",
        "- Buffer control\n",
        "- File rotation\n",
        "- Worker tuning\n",
        "\n",
        "Your task is to **make the pipeline as fast as possible**, while keeping the results correct.\n",
        "\n",
        "\n",
        "\n",
        "### **What you’ll need**\n",
        "\n",
        "- API base: `https://jaffle-shop.scalevector.ai/api/v1`\n",
        "- Docs: [https://jaffle-shop.scalevector.ai/docs](https://jaffle-shop.scalevector.ai/docs)\n",
        "- Start with these endpoints:\n",
        "  - `/customers`\n",
        "  - `/orders`\n",
        "  - `/products`\n",
        "\n",
        "Each of them returns **paged responses** — so you'll need to handle pagination.\n",
        "\n",
        "\n",
        "\n",
        "### **What to implement**\n",
        "\n",
        "1. **Extract** from the API using `dlt`\n",
        "   - Use `dlt.resource` and [`RESTClient`](https://dlthub.com/docs/devel/general-usage/http/rest-client) with proper pagination\n",
        "\n",
        "2. **Apply all performance techniques**\n",
        "   - Group resources into sources\n",
        "   - Yield **chunks/pages**, not single rows\n",
        "   - Use `parallelized=True`\n",
        "   - Set `EXTRACT__WORKERS`, `NORMALIZE__WORKERS`, and `LOAD__WORKERS`\n",
        "   - Tune buffer sizes and enable **file rotation**\n",
        "\n",
        "3. **Measure performance**\n",
        "   - Time the extract, normalize, and load stages separately\n",
        "   - Compare a naive version vs. optimized version\n",
        "   - Log thread info or `pipeline.last_trace` if helpful\n",
        "\n",
        "\n",
        "### **Deliverables**\n",
        "\n",
        "Share your code as a Google Colab or [GitHub Gist](https://gist.github.com/) in Homework Google Form. **This step is required for certification.**\n",
        "\n",
        "\n",
        "It should include:\n",
        "- Working pipeline for at least 2 endpoints\n",
        "- Before/after timing comparison\n",
        "- A short explanation of what changes made the biggest difference if there're any differences\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n1tcJPpBxrhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install \"dlt[sql_database, duckdb]\"\n",
        "!pip install pymysql\n",
        "!pip install pyyaml"
      ],
      "metadata": {
        "id": "fCezm6gYxqBy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Base Version Pipeline - No Optimization**"
      ],
      "metadata": {
        "id": "uFm0PHBgUgy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dlt\n",
        "from dlt.sources.helpers.rest_client import RESTClient\n",
        "from dlt.sources.helpers.rest_client.paginators import HeaderLinkPaginator\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "client = RESTClient(\n",
        "    base_url=\"https://jaffle-shop.scalevector.ai/api/v1\",\n",
        "    paginator=HeaderLinkPaginator(links_next_key=\"next\")\n",
        ")\n",
        "\n",
        "\n",
        "@dlt.resource(table_name=\"customers\", write_disposition=\"replace\")\n",
        "def get_customers():\n",
        "    logger.info(\"Starting extraction of customers data\")\n",
        "    paginator = client.paginate(\"customers\", params={\"page\":1, \"page_size\":1000})\n",
        "    for page in paginator:\n",
        "        yield page\n",
        "    logger.info(\"Completed extraction of customers data\")\n",
        "\n",
        "@dlt.resource(table_name=\"orders\", write_disposition=\"replace\")\n",
        "def get_orders():\n",
        "    logger.info(\"Starting extraction of orders data\")\n",
        "    paginator = client.paginate(\"orders\", params={\"page\":1, \"page_size\":500})\n",
        "    for page in paginator:\n",
        "        yield page\n",
        "\n",
        "@dlt.resource(table_name=\"products\", write_disposition=\"replace\")\n",
        "def get_products():\n",
        "    logger.info(\"Starting extraction of products data\")\n",
        "    paginator = client.paginate(\"products\", params={\"page\":1, \"page_size\":100})\n",
        "    for page in paginator:\n",
        "        yield page\n",
        "    logger.info(\"Completed extraction of products data\")\n",
        "    return\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        pipeline = dlt.pipeline(\n",
        "            pipeline_name=\"jaffle_shop_pipeline_v1\",\n",
        "            destination=\"duckdb\",\n",
        "            dataset_name=\"jaffle_shop\",\n",
        "            #progress=\"log\"\n",
        "        )\n",
        "\n",
        "        load_info = pipeline.run([get_customers, get_orders, get_products])\n",
        "\n",
        "        print(f\"{pipeline.last_trace}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Pipeline failed with error: {str(e)}\", exc_info=True)\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iSOrUW0Unk2",
        "outputId": "9f7ae201-0aba-4530-f40c-0fd8ede4d1df"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run started at 2025-05-08 09:58:49.103423+00:00 and COMPLETED in 9 minutes and 16.99 seconds with 4 steps.\n",
            "Step extract COMPLETED in 8 minutes and 49.63 seconds.\n",
            "\n",
            "Load package 1746698329.2022405 is EXTRACTED and NOT YET LOADED to the destination and contains no failed jobs\n",
            "\n",
            "Step normalize COMPLETED in 12.09 seconds.\n",
            "Normalized data for the following tables:\n",
            "- _dlt_pipeline_state: 1 row(s)\n",
            "- customers: 935 row(s)\n",
            "- orders: 61948 row(s)\n",
            "- orders__items: 90900 row(s)\n",
            "- products: 10 row(s)\n",
            "\n",
            "Load package 1746698329.2022405 is NORMALIZED and NOT YET LOADED to the destination and contains no failed jobs\n",
            "\n",
            "Step load COMPLETED in 15.17 seconds.\n",
            "Pipeline jaffle_shop_pipeline_v1 load step completed in 15.15 seconds\n",
            "1 load package(s) were loaded to destination duckdb and into dataset jaffle_shop\n",
            "The duckdb destination used duckdb:////content/jaffle_shop_pipeline_v1.duckdb location to store data\n",
            "Load package 1746698329.2022405 is LOADED and contains no failed jobs\n",
            "\n",
            "Step run COMPLETED in 9 minutes and 16.98 seconds.\n",
            "Pipeline jaffle_shop_pipeline_v1 load step completed in 15.15 seconds\n",
            "1 load package(s) were loaded to destination duckdb and into dataset jaffle_shop\n",
            "The duckdb destination used duckdb:////content/jaffle_shop_pipeline_v1.duckdb location to store data\n",
            "Load package 1746698329.2022405 is LOADED and contains no failed jobs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The non-optimized version of the pipeline takes more than 9 minutes!*"
      ],
      "metadata": {
        "id": "jA7hbn2VbqLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Optimized Pipelines**"
      ],
      "metadata": {
        "id": "LKuJ734VVYlp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **V2 - Using Parallelized Resources and Grouping Resources**"
      ],
      "metadata": {
        "id": "SVkeog4EVeYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dlt\n",
        "from dlt.sources.helpers.rest_client import RESTClient\n",
        "from dlt.sources.helpers.rest_client.paginators import HeaderLinkPaginator\n",
        "import logging\n",
        "\n",
        "# Create a logger\n",
        "logger = logging.getLogger('dlt')\n",
        "\n",
        "client = RESTClient(\n",
        "    base_url=\"https://jaffle-shop.scalevector.ai/api/v1\",\n",
        "    paginator=HeaderLinkPaginator(links_next_key=\"next\")\n",
        ")\n",
        "\n",
        "def limit_pages(paginator, resource_name, limit=None):\n",
        "    \"\"\"Helper function to limit the number of pages returned from pagination\n",
        "\n",
        "    If limit is None, returns all pages from the paginator.\n",
        "    \"\"\"\n",
        "    page_count = 0\n",
        "    for page in paginator:\n",
        "        logger.info(f\"Retrieved page {page_count + 1} for {resource_name}\")\n",
        "        yield page\n",
        "        page_count += 1\n",
        "        # Only check limit if it's not None\n",
        "        if limit is not None and page_count >= limit:\n",
        "            logger.info(f\"Reached page limit of {limit} for {resource_name}\")\n",
        "            break\n",
        "\n",
        "    logger.info(f\"Total pages processed for {resource_name}: {page_count}\")\n",
        "\n",
        "@dlt.resource(table_name=\"customers\", write_disposition=\"replace\", parallelized=True)\n",
        "def get_customers():\n",
        "    logger.info(\"Starting extraction of customers data\")\n",
        "    paginator = client.paginate(\"customers\", params={\"page\":1, \"page_size\":1000})\n",
        "    for page in paginator:\n",
        "        yield page\n",
        "    logger.info(\"Completed extraction of customers data\")\n",
        "\n",
        "@dlt.resource(table_name=\"orders\", write_disposition=\"replace\", parallelized=True)\n",
        "def get_orders():\n",
        "    logger.info(\"Starting extraction of orders data\")\n",
        "    paginator = client.paginate(\"orders\", params={\"page\":1, \"page_size\":500})\n",
        "    for page in paginator:\n",
        "        yield page\n",
        "    logger.info(\"Completed extraction of orders data\")\n",
        "\n",
        "@dlt.resource(table_name=\"products\", write_disposition=\"replace\", parallelized=True)\n",
        "def get_products():\n",
        "    logger.info(\"Starting extraction of products data\")\n",
        "    paginator = client.paginate(\"products\", params={\"page\":1, \"page_size\":100})\n",
        "    for page in paginator:\n",
        "        yield page\n",
        "    logger.info(\"Completed extraction of products data\")\n",
        "\n",
        "@dlt.source\n",
        "def jaffle_shop_source():\n",
        "    logger.info(\"Initializing jaffle shop data source\")\n",
        "    return get_customers, get_orders, get_products\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        pipeline = dlt.pipeline(\n",
        "            pipeline_name=\"jaffle_shop_pipeline_v2\",\n",
        "            destination=\"duckdb\",\n",
        "            dataset_name=\"jaffle_shop\",\n",
        "            #progress=\"log\"\n",
        "        )\n",
        "\n",
        "\n",
        "        load_info = pipeline.run(jaffle_shop_source())\n",
        "\n",
        "        print(f\"{pipeline.last_trace}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Pipeline failed with error: {str(e)}\", exc_info=True)\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8Cnr2-CxwEx",
        "outputId": "d965c011-9ecb-402b-f4f7-98945c9c5ecc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run started at 2025-05-08 10:08:06.522638+00:00 and COMPLETED in 9 minutes and 8.03 seconds with 4 steps.\n",
            "Step extract COMPLETED in 8 minutes and 42.12 seconds.\n",
            "\n",
            "Load package 1746698886.6162472 is EXTRACTED and NOT YET LOADED to the destination and contains no failed jobs\n",
            "\n",
            "Step normalize COMPLETED in 12.09 seconds.\n",
            "Normalized data for the following tables:\n",
            "- customers: 935 row(s)\n",
            "- orders: 61948 row(s)\n",
            "- orders__items: 90900 row(s)\n",
            "- products: 10 row(s)\n",
            "\n",
            "Load package 1746698886.6162472 is NORMALIZED and NOT YET LOADED to the destination and contains no failed jobs\n",
            "\n",
            "Step load COMPLETED in 13.76 seconds.\n",
            "Pipeline jaffle_shop_pipeline_v2 load step completed in 13.72 seconds\n",
            "1 load package(s) were loaded to destination duckdb and into dataset jaffle_shop\n",
            "The duckdb destination used duckdb:////content/jaffle_shop_pipeline_v2.duckdb location to store data\n",
            "Load package 1746698886.6162472 is LOADED and contains no failed jobs\n",
            "\n",
            "Step run COMPLETED in 9 minutes and 8.03 seconds.\n",
            "Pipeline jaffle_shop_pipeline_v2 load step completed in 13.72 seconds\n",
            "1 load package(s) were loaded to destination duckdb and into dataset jaffle_shop\n",
            "The duckdb destination used duckdb:////content/jaffle_shop_pipeline_v2.duckdb location to store data\n",
            "Load package 1746698886.6162472 is LOADED and contains no failed jobs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*V1 has just a slight improvement*"
      ],
      "metadata": {
        "id": "KYQoxbEIcACI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **V3 - Larger In-memory Buffer + Increased Workers**"
      ],
      "metadata": {
        "id": "S9Lj-U0NcGev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dlt\n",
        "from dlt.sources.helpers.rest_client import RESTClient\n",
        "from dlt.sources.helpers.rest_client.paginators import HeaderLinkPaginator\n",
        "import logging\n",
        "import sys\n",
        "import os\n",
        "\n",
        "os.environ['EXTRACT__WORKERS'] = '8'\n",
        "os.environ['NORMALIZE__WORKERS'] = '2'\n",
        "os.environ['DATA_WRITER__BUFFER_MAX_ITEMS'] = '15000'\n",
        "\n",
        "# Create a logger\n",
        "logger = logging.getLogger('dlt')\n",
        "\n",
        "# Configure the logger for both console and file output\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Create a file handler with more detailed formatting\n",
        "file_handler = logging.FileHandler('dlt.log')\n",
        "file_handler.setLevel(logging.INFO)\n",
        "file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "file_handler.setFormatter(file_formatter)\n",
        "\n",
        "# Create a console handler\n",
        "console_handler = logging.StreamHandler(sys.stdout)\n",
        "console_handler.setLevel(logging.INFO)\n",
        "console_formatter = logging.Formatter('%(levelname)s: %(message)s')\n",
        "console_handler.setFormatter(console_formatter)\n",
        "\n",
        "# Add the handlers to the logger\n",
        "logger.addHandler(file_handler)\n",
        "logger.addHandler(console_handler)\n",
        "\n",
        "client = RESTClient(\n",
        "    base_url=\"https://jaffle-shop.scalevector.ai/api/v1\",\n",
        "    paginator=HeaderLinkPaginator(links_next_key=\"next\")\n",
        ")\n",
        "\n",
        "def limit_pages(paginator, resource_name, limit=None):\n",
        "    \"\"\"Helper function to limit the number of pages returned from pagination\n",
        "\n",
        "    If limit is None, returns all pages from the paginator.\n",
        "    \"\"\"\n",
        "    page_count = 0\n",
        "    for page in paginator:\n",
        "        logger.info(f\"Retrieved page {page_count + 1} for {resource_name}\")\n",
        "        yield page\n",
        "        page_count += 1\n",
        "        # Only check limit if it's not None\n",
        "        if limit is not None and page_count >= limit:\n",
        "            logger.info(f\"Reached page limit of {limit} for {resource_name}\")\n",
        "            break\n",
        "\n",
        "    logger.info(f\"Total pages processed for {resource_name}: {page_count}\")\n",
        "\n",
        "@dlt.resource(table_name=\"customers\", write_disposition=\"replace\", parallelized=True)\n",
        "def get_customers():\n",
        "    logger.info(\"Starting extraction of customers data\")\n",
        "    paginator = client.paginate(\"customers\", params={\"page\":1, \"page_size\":1000})\n",
        "    for page in paginator:\n",
        "        yield page\n",
        "    logger.info(\"Completed extraction of customers data\")\n",
        "\n",
        "@dlt.resource(table_name=\"orders\", write_disposition=\"replace\", parallelized=True)\n",
        "def get_orders():\n",
        "    logger.info(\"Starting extraction of orders data\")\n",
        "    paginator = client.paginate(\"orders\", params={\"page\":1, \"page_size\":500})\n",
        "    for page in paginator:\n",
        "      yield page\n",
        "    logger.info(\"Completed extraction of orders data\")\n",
        "\n",
        "@dlt.resource(table_name=\"products\", write_disposition=\"replace\", parallelized=True)\n",
        "def get_products():\n",
        "    logger.info(\"Starting extraction of products data\")\n",
        "    paginator = client.paginate(\"products\", params={\"page\":1, \"page_size\":100})\n",
        "    for page in paginator:\n",
        "        yield page\n",
        "    logger.info(\"Completed extraction of products data\")\n",
        "\n",
        "@dlt.source\n",
        "def jaffle_shop_source():\n",
        "    logger.info(\"Initializing jaffle shop data source\")\n",
        "    return get_customers, get_orders, get_products\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        pipeline = dlt.pipeline(\n",
        "            pipeline_name=\"jaffle_shop_pipeline_v3\",\n",
        "            destination=\"duckdb\",\n",
        "            dataset_name=\"jaffle_shop\",\n",
        "            #progress=\"log\"\n",
        "        )\n",
        "\n",
        "\n",
        "        load_info = pipeline.run(jaffle_shop_source())\n",
        "\n",
        "        print(f\"{pipeline.last_trace}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Pipeline failed with error: {str(e)}\", exc_info=True)\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0utVN5ZEVq3g",
        "outputId": "7326a7d7-d3c6-443e-ec90-4518d14403c6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run started at 2025-05-08 10:43:47.284910+00:00 and COMPLETED in 9 minutes and 1.21 seconds with 4 steps.\n",
            "Step extract COMPLETED in 8 minutes and 35.31 seconds.\n",
            "\n",
            "Load package 1746701027.3618095 is EXTRACTED and NOT YET LOADED to the destination and contains no failed jobs\n",
            "\n",
            "Step normalize COMPLETED in 11.52 seconds.\n",
            "Normalized data for the following tables:\n",
            "- customers: 935 row(s)\n",
            "- orders: 61948 row(s)\n",
            "- orders__items: 90900 row(s)\n",
            "- products: 10 row(s)\n",
            "\n",
            "Load package 1746701027.3618095 is NORMALIZED and NOT YET LOADED to the destination and contains no failed jobs\n",
            "\n",
            "Step load COMPLETED in 14.31 seconds.\n",
            "Pipeline jaffle_shop_pipeline_v3 load step completed in 13.83 seconds\n",
            "1 load package(s) were loaded to destination duckdb and into dataset jaffle_shop\n",
            "The duckdb destination used duckdb:////content/jaffle_shop_pipeline_v3.duckdb location to store data\n",
            "Load package 1746701027.3618095 is LOADED and contains no failed jobs\n",
            "\n",
            "Step run COMPLETED in 9 minutes and 1.21 seconds.\n",
            "Pipeline jaffle_shop_pipeline_v3 load step completed in 13.83 seconds\n",
            "1 load package(s) were loaded to destination duckdb and into dataset jaffle_shop\n",
            "The duckdb destination used duckdb:////content/jaffle_shop_pipeline_v3.duckdb location to store data\n",
            "Load package 1746701027.3618095 is LOADED and contains no failed jobs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deployment"
      ],
      "metadata": {
        "id": "XQH6Wz3Bgj3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install \"dlt[cli]\""
      ],
      "metadata": {
        "id": "GMeQJuKMgiXW"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V3Y6SALPcvnw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}